{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbLHTNfSclli"
   },
   "source": [
    "**Секція 1. Логістична регресія з нуля.**\n",
    "\n",
    "Будемо крок за кроком будувати модель лог регресії з нуля для передбачення, чи буде врожай більше за 80 яблук (задача подібна до лекційної, але на класифікацію).\n",
    "\n",
    "Давайте нагадаємо основні формули для логістичної регресії.\n",
    "\n",
    "### Функція гіпотези - обчислення передбачення у логістичній регресії:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(x W^T + b) = \\frac{1}{1 + e^{-(x W^T + b)}}\n",
    "$$\n",
    "\n",
    "Де:\n",
    "- $ \\hat{y} $ — це ймовірність \"позитивного\" класу.\n",
    "- $ x $ — це вектор (або матриця для набору прикладів) вхідних даних.\n",
    "- $ W $ — це вектор (або матриця) вагових коефіцієнтів моделі.\n",
    "- $ b $ — це зміщення (bias).\n",
    "- $ \\sigma(z) $ — це сигмоїдна функція активації.\n",
    "\n",
    "### Як обчислюється сигмоїдна функція:\n",
    "\n",
    "Сигмоїдна функція $ \\sigma(z) $ має вигляд:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Ця функція перетворює будь-яке дійсне значення $ z $ в інтервал від 0 до 1, що дозволяє інтерпретувати вихід як ймовірність для логістичної регресії.\n",
    "\n",
    "### Формула функції втрат для логістичної регресії (бінарна крос-ентропія):\n",
    "\n",
    "Функція втрат крос-ентропії оцінює, наскільки добре модель передбачає класи, порівнюючи передбачені ймовірності $ \\hat{y} $ із справжніми мітками $ y $. Формула наступна:\n",
    "\n",
    "$$\n",
    "L(y, \\hat{y}) = - \\left[ y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y}) \\right]\n",
    "$$\n",
    "\n",
    "Де:\n",
    "- $ y $ — це справжнє значення (мітка класу, 0 або 1).\n",
    "- $ \\hat{y} $ — це передбачене значення (ймовірність).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtOYB-RHfc_r"
   },
   "source": [
    "1.\n",
    "Тут вже наведений код для ініціювання набору даних в форматі numpy. Перетворіть `inputs`, `targets` на `torch` тензори. Виведіть результат на екран."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3BNXSR-VdYKQ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QLKZ77x4v_-v"
   },
   "outputs": [],
   "source": [
    "# Вхідні дані (temp, rainfall, humidity)\n",
    "inputs = np.array([[73, 67, 43],\n",
    "                   [91, 88, 64],\n",
    "                   [87, 134, 58],\n",
    "                   [102, 43, 37],\n",
    "                   [69, 96, 70]], dtype='float32')\n",
    "\n",
    "# Таргети (apples > 80)\n",
    "targets = np.array([[0],\n",
    "                    [1],\n",
    "                    [1],\n",
    "                    [0],\n",
    "                    [1]], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "KjoeaDrk6fO7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вхідні дані як тензор:\n",
      "tensor([[ 73.,  67.,  43.],\n",
      "        [ 91.,  88.,  64.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [102.,  43.,  37.],\n",
      "        [ 69.,  96.,  70.]])\n",
      "Таргети як тензор:\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "# Перетворення на PyTorch тензори\n",
    "inputs_tensor = torch.from_numpy(inputs)\n",
    "targets_tensor = torch.from_numpy(targets)\n",
    "\n",
    "# Виведення результату\n",
    "print(\"Вхідні дані як тензор:\")\n",
    "print(inputs_tensor)\n",
    "\n",
    "print(\"Таргети як тензор:\")\n",
    "print(targets_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKzbJKfOgGV8"
   },
   "source": [
    "2. Ініціюйте ваги `w`, `b` для моделі логістичної регресії потрібної форми зважаючи на розмірності даних випадковими значеннями з нормального розподілу. Лишаю тут код для фіксації `random_seed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "aXhKw6Tdj1-d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2121d9450d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.random.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "eApcB7eb6h9o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ваги w:\n",
      "tensor([[0.6614],\n",
      "        [0.2669],\n",
      "        [0.0617]], requires_grad=True)\n",
      "Зсув b:\n",
      "tensor([0.6213], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Розмірність вхідних даних\n",
    "input_size = inputs.shape[1]  # Кількість ознак\n",
    "\n",
    "# Ініціалізація ваг і зсуву з нормального розподілу\n",
    "w = torch.randn(input_size, 1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "\n",
    "print(\"Ваги w:\")\n",
    "print(w)\n",
    "\n",
    "print(\"Зсув b:\")\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYGxNGTaf5s6"
   },
   "source": [
    "3. Напишіть функцію `model`, яка буде обчислювати функцію гіпотези в логістичній регресії і дозволяти робити передбачення на основі введеного рядка даних і коефіцієнтів в змінних `w`, `b`.\n",
    "\n",
    "  **Важливий момент**, що функція `model` робить обчислення на `torch.tensors`, тож для математичних обчислень використовуємо фукнціонал `torch`, наприклад:\n",
    "  - обчсилення $e^x$: `torch.exp(x)`\n",
    "  - обчсилення $log(x)$: `torch.log(x)`\n",
    "  - обчислення середнього значення вектору `x`: `torch.mean(x)`\n",
    "\n",
    "  Використайте функцію `model` для обчислення передбачень з поточними значеннями `w`, `b`.Виведіть результат обчислень на екран.\n",
    "\n",
    "  Проаналізуйте передбачення. Чи не викликають вони у вас підозр? І якщо викликають, то чим це може бути зумовлено?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "pSz2j4Fh6jBv"
   },
   "outputs": [],
   "source": [
    "def model(x, w, b):\n",
    "    \"\"\"\n",
    "    Функція для обчислення виходу логістичної регресії.\n",
    "\n",
    "    Args:\n",
    "        x: Вхідний тензор даних.\n",
    "        w: Вектор ваг.\n",
    "        b: Зсув.\n",
    "\n",
    "    Returns:\n",
    "        Тензор, що містить ймовірності належності до позитивного класу.\n",
    "    \"\"\"\n",
    "\n",
    "    # Обчислення лінійної комбінації\n",
    "    linear = torch.matmul(x, w) + b\n",
    "\n",
    "    # Застосування сигмоїди для отримання ймовірності\n",
    "    y_predicted = torch.sigmoid(linear)\n",
    "\n",
    "    return y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Передбачення:\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]], grad_fn=<SigmoidBackward0>)\n",
      "Таргети як тензор:\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "# Обчислення передбачення\n",
    "predictions = model(inputs_tensor, w, b)\n",
    "print(\"Передбачення:\")\n",
    "print(predictions)\n",
    "\n",
    "print(\"Таргети як тензор:\")\n",
    "print(targets_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Спостереження:**\n",
    "    Всі передбачення в нас ідентифікувалися як позитивний клас - \"1\", хоча в оригінальних таргетах бачимо і негативні класи\n",
    "\n",
    "Це може бути пов\"язано з вагами, або неправильна функція втрат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O2AGM0Mb2yHa"
   },
   "source": [
    "4. Напишіть функцію `binary_cross_entropy`, яка приймає на вхід передбачення моделі `predicted_probs` та справжні мітки в даних `true_labels` і обчислює значення втрат (loss)  за формулою бінарної крос-ентропії для кожного екземпляра та вертає середні втрати по всьому набору даних.\n",
    "  Використайте функцію `binary_cross_entropy` для обчислення втрат для поточних передбачень моделі."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1bWlovvx6kZS"
   },
   "outputs": [],
   "source": [
    "def binary_cross_entropy(predicted_probs, true_labels):\n",
    "  \"\"\"\n",
    "  Обчислює бінарну крос-ентропію для кожного зразка та повертає середнє значення.\n",
    "\n",
    "  Args:\n",
    "    predicted_probs: Тензор ймовірностей, передбачених моделлю.\n",
    "    true_labels: Тензор справжніх міток (0 або 1).\n",
    "\n",
    "  Returns:\n",
    "    Середнє значення бінарної крос-ентропії.\n",
    "  \"\"\"\n",
    "\n",
    "  # Запобігаємо логарифмуванню нуля\n",
    "  epsilon = 1e-8\n",
    "  predicted_probs = torch.clamp(predicted_probs, epsilon, 1 - epsilon)\n",
    "\n",
    "  # Обчислення бінарної крос-ентропії для кожного зразка\n",
    "  bce = -1 * (true_labels * torch.log(predicted_probs) + (1 - true_labels) * torch.log(1 - predicted_probs))\n",
    "\n",
    "  # Обчислення середнього значення втрат\n",
    "  loss = torch.mean(bce)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Середні втрати: tensor(nan, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = binary_cross_entropy(predictions, targets_tensor)\n",
    "print(\"Середні втрати:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Знайдено NaN у тензорі втрат\n"
     ]
    }
   ],
   "source": [
    "if torch.isnan(loss).any():\n",
    "    print(\"Знайдено NaN у тензорі втрат\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFKpQxdHi1__"
   },
   "source": [
    "5. Зробіть зворотнє поширення помилки і виведіть градієнти за параметрами `w`, `b`. Проаналізуйте їх значення. Як гадаєте, чому вони саме такі?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "YAbXUNSJ6mCl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Градієнт для w: tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]])\n",
      "Градієнт для b: tensor([nan])\n"
     ]
    }
   ],
   "source": [
    "# Обчислення градієнтів\n",
    "loss.backward()\n",
    "\n",
    "# Виведення градієнтів\n",
    "print(\"Градієнт для w:\", w.grad)\n",
    "print(\"Градієнт для b:\", b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDN1t1RujQsK"
   },
   "source": [
    "**Що сталось?**\n",
    "\n",
    "В цій задачі, коли ми ініціювали значення випадковими значеннями з нормального розподілу, насправді ці значення не були дуже гарними стартовими значеннями і привели до того, що градієнти стали дуже малими або навіть рівними нулю (це призводить до того, що градієнти \"зникають\"), і відповідно при оновленні ваг у нас не буде нічого змінюватись. Це називається `gradient vanishing`. Це відбувається через **насичення сигмоїдної функції активації.**\n",
    "\n",
    "У нашій задачі ми використовуємо сигмоїдну функцію активації, яка має такий вигляд:\n",
    "\n",
    "   $$\n",
    "   \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "   $$\n",
    "\n",
    "\n",
    "Коли значення $z$ дуже велике або дуже мале, сигмоїдна функція починає \"насичуватись\". Це означає, що для великих позитивних $z$ сигмоїда наближається до 1, а для великих негативних — до 0. В цих діапазонах градієнти починають стрімко зменшуватись і наближаються до нуля (бо градієнт - це похідна, похідна на проміжку функції, де вона паралельна осі ОХ, дорівнює 0), що робить оновлення ваг неможливим.\n",
    "\n",
    "![](https://editor.analyticsvidhya.com/uploads/27889vaegp.png)\n",
    "\n",
    "У логістичній регресії $ z = x \\cdot w + b $. Якщо ваги $w, b$ - великі, значення $z$ також буде великим, і сигмоїда перейде в насичену область, де градієнти дуже малі.\n",
    "\n",
    "Саме це сталося в нашій задачі, де великі випадкові значення ваг викликали насичення сигмоїдної функції. Це в свою чергу призводить до того, що під час зворотного поширення помилки (backpropagation) модель оновлює ваги дуже повільно або зовсім не оновлює. Це називається проблемою **зникнення градієнтів** (gradient vanishing problem).\n",
    "\n",
    "**Що ж робити?**\n",
    "Ініціювати ваги маленькими значеннями навколо нуля. Наприклад ми можемо просто в існуючій ініціалізації ваги розділити на 1000. Можна також використати інший спосіб ініціалізації вагів - інформація про це [тут](https://www.geeksforgeeks.org/initialize-weights-in-pytorch/).\n",
    "\n",
    "Як це робити - показую нижче. **Виконайте код та знову обчисліть передбачення, лосс і виведіть градієнти.**\n",
    "\n",
    "А я пишу пояснення, чому просто не зробити\n",
    "\n",
    "```\n",
    "w = torch.randn(1, 3, requires_grad=True)/1000\n",
    "b = torch.randn(1, requires_grad=True)/1000\n",
    "```\n",
    "\n",
    "Нам потрібно, аби тензори вагів були листовими (leaf tensors).\n",
    "\n",
    "1. **Що таке листовий тензор**\n",
    "Листовий тензор — це тензор, який був створений користувачем безпосередньо і з якого починається обчислювальний граф. Якщо такий тензор має `requires_grad=True`, PyTorch буде відслідковувати всі операції, виконані над ним, щоб правильно обчислювати градієнти під час навчання.\n",
    "\n",
    "2. **Чому ми використовуємо `w.data` замість звичайних операцій**\n",
    "Якщо ми просто виконали б операції, такі як `(w - 0.5) / 100`, ми б отримали **новий тензор**, який вже не був би листовим тензором, оскільки ці операції створюють **новий** тензор, а не модифікують існуючий.\n",
    "\n",
    "  Проте, щоб залишити наші тензори ваги `w` та зміщення `b` листовими і продовжити можливість відстеження градієнтів під час тренування, ми використовуємо атрибут `.data`. Цей атрибут дозволяє **виконувати операції in-place (прямо на існуючому тензорі)** без зміни самого об'єкта тензора. Отже, тензор залишається листовим, і PyTorch може коректно обчислювати його градієнти.\n",
    "\n",
    "3. **Чому важливо залишити тензор листовим**\n",
    "Якщо тензор більше не є листовим (наприклад, через проведення операцій, що створюють нові тензори), ви не зможете отримати градієнти за допомогою `w.grad` чи `b.grad` після виклику `loss.backward()`. Це може призвести до втрати можливості оновлення параметрів під час тренування моделі. В нашому випадку ми хочемо, щоб тензори `w` та `b` накопичували градієнти, тому вони повинні залишатись листовими.\n",
    "\n",
    "**Висновок:**\n",
    "Ми використовуємо `.data`, щоб виконати операції зміни значень на ваги і зміщення **in-place**, залишаючи їх листовими тензорами, які можуть накопичувати градієнти під час навчання. Це дозволяє коректно працювати механізму зворотного поширення помилки (backpropagation) і оновлювати ваги моделі."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOPSQyttpVjO"
   },
   "source": [
    "5. Виконайте код та знову обчисліть передбачення, лосс і знайдіть градієнти та виведіть всі ці тензори на екран."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "-EBOJ3tsnRaD"
   },
   "outputs": [],
   "source": [
    "torch.random.manual_seed(1)\n",
    "w = torch.randn(1, 3, requires_grad=True)  # Листовий тензор\n",
    "b = torch.randn(1, requires_grad=True)     # Листовий тензор\n",
    "\n",
    "# in-place операції\n",
    "w.data = w.data / 1000\n",
    "b.data = b.data / 1000\n",
    "\n",
    "w = w.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "-JwXiSpX6orh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Передбачення:\n",
      "tensor([[0.5174],\n",
      "        [0.5220],\n",
      "        [0.5244],\n",
      "        [0.5204],\n",
      "        [0.5190]], grad_fn=<SigmoidBackward0>)\n",
      "Таргети як тензор:\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]])\n",
      "Середні втрати: tensor(0.6829, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Обчислення передбачення\n",
    "predictions_2 = model(inputs_tensor, w, b)\n",
    "\n",
    "loss_2 = binary_cross_entropy(predictions_2, targets_tensor)\n",
    "print(\"Передбачення:\")\n",
    "print(predictions_2)\n",
    "\n",
    "print(\"Таргети як тензор:\")\n",
    "print(targets_tensor)\n",
    "\n",
    "print(\"Середні втрати:\", loss_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCdi44IT334o"
   },
   "source": [
    "6. Напишіть алгоритм градієнтного спуску, який буде навчати модель з використанням написаних раніше функцій і виконуючи оновлення ваг. Алгоритм має включати наступні кроки:\n",
    "\n",
    "  1. Генерація прогнозів\n",
    "  2. Обчислення втрат\n",
    "  3. Обчислення градієнтів (gradients) loss-фукнції відносно ваг і зсувів\n",
    "  4. Налаштування ваг шляхом віднімання невеликої величини, пропорційної градієнту (`learning_rate` домножений на градієнт)\n",
    "  5. Скидання градієнтів на нуль\n",
    "\n",
    "Виконайте градієнтний спуск протягом 1000 епох, обчисліть фінальні передбачення і проаналізуйте, чи вони точні?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "mObHPyE06qsO"
   },
   "outputs": [],
   "source": [
    "def gradient_descent( inputs_tensor, targets_tensor, learning_rate, num_epochs):\n",
    "  \"\"\"\n",
    "  Функція реалізує алгоритм градієнтного спуску.\n",
    "\n",
    "  Args:\n",
    "    model: Модель, яку необхідно навчити.\n",
    "    inputs_tensor: Вхідні дані.\n",
    "    targets_tensor: Бажані виходи.\n",
    "    learning_rate: Швидкість навчання.\n",
    "    num_epochs: Кількість епох навчання.\n",
    "  \"\"\"\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    # Генерація прогнозів\n",
    "    predictions = model(inputs_tensor, w, b)\n",
    "\n",
    "    # Обчислення втрат\n",
    "    loss = binary_cross_entropy(predictions, targets_tensor)\n",
    "\n",
    "    # Обчислення градієнтів\n",
    "    loss.backward()\n",
    "\n",
    "    # Оновлення ваг\n",
    "    with torch.no_grad():\n",
    "      for param in model.parameters():\n",
    "        param -= learning_rate * param.grad\n",
    "\n",
    "    # Скидання градієнтів\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "      print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'parameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[0;32m      4\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mgradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 25\u001b[0m, in \u001b[0;36mgradient_descent\u001b[1;34m(inputs_tensor, targets_tensor, learning_rate, num_epochs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Оновлення ваг\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 25\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m():\n\u001b[0;32m     26\u001b[0m     param \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m param\u001b[38;5;241m.\u001b[39mgrad\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Скидання градієнтів\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'parameters'"
     ]
    }
   ],
   "source": [
    "# Приклад використання:\n",
    "#model_1 = model(inputs_tensor, w, b)\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1000\n",
    "\n",
    "gradient_descent(inputs_tensor, targets_tensor, learning_rate, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuRhlyF9qAia"
   },
   "source": [
    "**Секція 2. Створення лог регресії з використанням функціоналу `torch.nn`.**\n",
    "\n",
    "Давайте повторно реалізуємо ту ж модель, використовуючи деякі вбудовані функції та класи з PyTorch.\n",
    "\n",
    "Даних у нас буде побільше - тож, визначаємо нові масиви."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "IX8Bhm74rV4M"
   },
   "outputs": [],
   "source": [
    "# Вхідні дані (temp, rainfall, humidity)\n",
    "inputs = np.array([[73, 67, 43],\n",
    "                   [91, 88, 64],\n",
    "                   [87, 134, 58],\n",
    "                   [102, 43, 37],\n",
    "                   [69, 96, 70],\n",
    "                   [73, 67, 43],\n",
    "                   [91, 88, 64],\n",
    "                   [87, 134, 58],\n",
    "                   [102, 43, 37],\n",
    "                   [69, 96, 70],\n",
    "                   [73, 67, 43],\n",
    "                   [91, 88, 64],\n",
    "                   [87, 134, 58],\n",
    "                   [102, 43, 37],\n",
    "                   [69, 96, 70]], dtype='float32')\n",
    "\n",
    "# Таргети (apples > 80)\n",
    "targets = np.array([[0],\n",
    "                    [1],\n",
    "                    [1],\n",
    "                    [0],\n",
    "                    [1],\n",
    "                    [0],\n",
    "                    [1],\n",
    "                    [1],\n",
    "                    [0],\n",
    "                    [1],\n",
    "                    [0],\n",
    "                    [1],\n",
    "                    [1],\n",
    "                    [0],\n",
    "                    [1]], dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7X2dV30KtAPu"
   },
   "source": [
    "7. Завантажте вхідні дані та мітки в PyTorch тензори та з них створіть датасет, який поєднує вхідні дані з мітками, використовуючи клас `TensorDataset`. Виведіть перші 3 елементи в датасеті.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "chrvMfBs6vjo"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Перевірка перших 3 елементів датасету\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mdataset\u001b[49m[i])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Перетворення NumPy масивів в тензори PyTorch\n",
    "inputs_tensor = torch.from_numpy(inputs)\n",
    "targets_tensor = torch.from_numpy(targets)\n",
    "\n",
    "# Створення датасету\n",
    "train_ds = TensorDataset(inputs_tensor, targets_tensor)\n",
    "\n",
    "# Перевірка перших 3 елементів датасету\n",
    "for i in range(3):\n",
    "    print(dataset[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4nMFaa8suOd3"
   },
   "source": [
    "8. Визначте data loader з класом **DataLoader** для підготовленого датасету `train_ds`, встановіть розмір батчу на 5 та увімкніть перемішування даних для ефективного навчання моделі. Виведіть перший елемент в дата лоадері."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ZCsRo5Mx6wEI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 73.,  67.,  43.],\n",
      "        [ 91.,  88.,  64.],\n",
      "        [102.,  43.,  37.],\n",
      "        [ 69.,  96.,  70.],\n",
      "        [102.,  43.,  37.]])\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "# Створення DataLoader з розміром батчу 5 та перемішуванням даних\n",
    "train_loader = DataLoader(train_ds, batch_size=5, shuffle=True)\n",
    "\n",
    "# Виведення першого елементу\n",
    "for batch in train_loader:\n",
    "    inputs, targets = batch\n",
    "    print(inputs)\n",
    "    print(targets)\n",
    "    break  # Виводимо тільки перший батч"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymcQOo_hum6I"
   },
   "source": [
    "9. Створіть клас `LogReg` для логістичної регресії, наслідуючи модуль `torch.nn.Module` за прикладом в лекції (в частині про FeedForward мережі).\n",
    "\n",
    "  У нас модель складається з лінійної комбінації вхідних значень і застосування фукнції сигмоїда. Тож, нейромережа буде складатись з лінійного шару `nn.Linear` і використання активації `nn.Sigmid`. У створеному класі мають бути реалізовані методи `__init__` з ініціалізацією шарів і метод `forward` для виконання прямого проходу моделі через лінійний шар і функцію активації.\n",
    "\n",
    "  Створіть екземпляр класу `LogReg` в змінній `model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "EyAwhTBW6xxz"
   },
   "outputs": [],
   "source": [
    "class LogReg(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LogReg, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "        \n",
    "# Створення екземпляру класу\n",
    "model = LogReg(input_size=3, output_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RflV7xeVyoJy"
   },
   "source": [
    "10. Задайте оптимізатор `Stockastic Gradient Descent` в змінній `opt` для навчання моделі логістичної регресії. А також визначіть в змінній `loss` функцію втрат `binary_cross_entropy` з модуля `torch.nn.functional` для обчислення втрат моделі. Обчисліть втрати для поточних передбачень і міток, а потім виведіть їх. Зробіть висновок, чи моделі вдалось навчитись?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "3QCATPU_6yfa"
   },
   "outputs": [],
   "source": [
    "# Оптимізатор SGD\n",
    "opt = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Функція втрат бінарної крос-ентропії\n",
    "loss_fn = F.binary_cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 6.81674337387085\n"
     ]
    }
   ],
   "source": [
    "# обчислення втрат:\n",
    "outputs = model(inputs)  # Отримати прогнози моделі\n",
    "loss = loss_fn(outputs, targets)  # Обчислити втрати\n",
    "print(f\"Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Спостереження:**\n",
    "\n",
    "По функції втрат бачимо, що моделі вдалося навчитися"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ch-WrYnKzMzq"
   },
   "source": [
    "11. Візьміть з лекції функцію для тренування моделі з відстеженням значень втрат і навчіть щойно визначену модель на 1000 епохах. Виведіть після цього графік зміни loss, фінальні передбачення і значення таргетів."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "cEHQH9qE626k"
   },
   "outputs": [],
   "source": [
    "# Модифікована функцію fit для відстеження втрат\n",
    "def fit_return_loss(num_epochs, model, loss_fn, opt, train_dl):\n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # Ініціалізуємо акумулятор для втрат\n",
    "        total_loss = 0\n",
    "\n",
    "        for xb, yb in train_dl:\n",
    "            # Генеруємо передбачення\n",
    "            pred = model(xb)\n",
    "\n",
    "            # Обчислюємо втрати\n",
    "            loss = loss_fn(pred, yb)\n",
    "\n",
    "            # Виконуємо градієнтний спуск\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "            # Накопичуємо втрати\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Обчислюємо середні втрати для епохи\n",
    "        avg_loss = total_loss / len(train_dl)\n",
    "        losses.append(avg_loss)\n",
    "\n",
    "        # Виводимо підсумок епохи\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "          print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Loss: 40.0000\n",
      "Epoch [20/1000], Loss: 40.0000\n",
      "Epoch [30/1000], Loss: 40.0000\n",
      "Epoch [40/1000], Loss: 40.0000\n",
      "Epoch [50/1000], Loss: 40.0000\n",
      "Epoch [60/1000], Loss: 40.0000\n",
      "Epoch [70/1000], Loss: 40.0000\n",
      "Epoch [80/1000], Loss: 40.0000\n",
      "Epoch [90/1000], Loss: 40.0000\n",
      "Epoch [100/1000], Loss: 40.0000\n",
      "Epoch [110/1000], Loss: 40.0000\n",
      "Epoch [120/1000], Loss: 40.0000\n",
      "Epoch [130/1000], Loss: 40.0000\n",
      "Epoch [140/1000], Loss: 40.0000\n",
      "Epoch [150/1000], Loss: 40.0000\n",
      "Epoch [160/1000], Loss: 40.0000\n",
      "Epoch [170/1000], Loss: 40.0000\n",
      "Epoch [180/1000], Loss: 40.0000\n",
      "Epoch [190/1000], Loss: 40.0000\n",
      "Epoch [200/1000], Loss: 40.0000\n",
      "Epoch [210/1000], Loss: 40.0000\n",
      "Epoch [220/1000], Loss: 40.0000\n",
      "Epoch [230/1000], Loss: 40.0000\n",
      "Epoch [240/1000], Loss: 40.0000\n",
      "Epoch [250/1000], Loss: 40.0000\n",
      "Epoch [260/1000], Loss: 40.0000\n",
      "Epoch [270/1000], Loss: 40.0000\n",
      "Epoch [280/1000], Loss: 40.0000\n",
      "Epoch [290/1000], Loss: 40.0000\n",
      "Epoch [300/1000], Loss: 40.0000\n",
      "Epoch [310/1000], Loss: 40.0000\n",
      "Epoch [320/1000], Loss: 40.0000\n",
      "Epoch [330/1000], Loss: 40.0000\n",
      "Epoch [340/1000], Loss: 40.0000\n",
      "Epoch [350/1000], Loss: 40.0000\n",
      "Epoch [360/1000], Loss: 40.0000\n",
      "Epoch [370/1000], Loss: 40.0000\n",
      "Epoch [380/1000], Loss: 40.0000\n",
      "Epoch [390/1000], Loss: 40.0000\n",
      "Epoch [400/1000], Loss: 40.0000\n",
      "Epoch [410/1000], Loss: 40.0000\n",
      "Epoch [420/1000], Loss: 40.0000\n",
      "Epoch [430/1000], Loss: 40.0000\n",
      "Epoch [440/1000], Loss: 40.0000\n",
      "Epoch [450/1000], Loss: 40.0000\n",
      "Epoch [460/1000], Loss: 40.0000\n",
      "Epoch [470/1000], Loss: 40.0000\n",
      "Epoch [480/1000], Loss: 40.0000\n",
      "Epoch [490/1000], Loss: 40.0000\n",
      "Epoch [500/1000], Loss: 40.0000\n",
      "Epoch [510/1000], Loss: 40.0000\n",
      "Epoch [520/1000], Loss: 40.0000\n",
      "Epoch [530/1000], Loss: 40.0000\n",
      "Epoch [540/1000], Loss: 40.0000\n",
      "Epoch [550/1000], Loss: 40.0000\n",
      "Epoch [560/1000], Loss: 40.0000\n",
      "Epoch [570/1000], Loss: 40.0000\n",
      "Epoch [580/1000], Loss: 40.0000\n",
      "Epoch [590/1000], Loss: 40.0000\n",
      "Epoch [600/1000], Loss: 40.0000\n",
      "Epoch [610/1000], Loss: 40.0000\n",
      "Epoch [620/1000], Loss: 40.0000\n",
      "Epoch [630/1000], Loss: 40.0000\n",
      "Epoch [640/1000], Loss: 40.0000\n",
      "Epoch [650/1000], Loss: 40.0000\n",
      "Epoch [660/1000], Loss: 40.0000\n",
      "Epoch [670/1000], Loss: 40.0000\n",
      "Epoch [680/1000], Loss: 40.0000\n",
      "Epoch [690/1000], Loss: 40.0000\n",
      "Epoch [700/1000], Loss: 40.0000\n",
      "Epoch [710/1000], Loss: 40.0000\n",
      "Epoch [720/1000], Loss: 40.0000\n",
      "Epoch [730/1000], Loss: 40.0000\n",
      "Epoch [740/1000], Loss: 40.0000\n",
      "Epoch [750/1000], Loss: 40.0000\n",
      "Epoch [760/1000], Loss: 40.0000\n",
      "Epoch [770/1000], Loss: 40.0000\n",
      "Epoch [780/1000], Loss: 40.0000\n",
      "Epoch [790/1000], Loss: 40.0000\n",
      "Epoch [800/1000], Loss: 40.0000\n",
      "Epoch [810/1000], Loss: 40.0000\n",
      "Epoch [820/1000], Loss: 40.0000\n",
      "Epoch [830/1000], Loss: 40.0000\n",
      "Epoch [840/1000], Loss: 40.0000\n",
      "Epoch [850/1000], Loss: 40.0000\n",
      "Epoch [860/1000], Loss: 40.0000\n",
      "Epoch [870/1000], Loss: 40.0000\n",
      "Epoch [880/1000], Loss: 40.0000\n",
      "Epoch [890/1000], Loss: 40.0000\n",
      "Epoch [900/1000], Loss: 40.0000\n",
      "Epoch [910/1000], Loss: 40.0000\n",
      "Epoch [920/1000], Loss: 40.0000\n",
      "Epoch [930/1000], Loss: 40.0000\n",
      "Epoch [940/1000], Loss: 40.0000\n",
      "Epoch [950/1000], Loss: 40.0000\n",
      "Epoch [960/1000], Loss: 40.0000\n",
      "Epoch [970/1000], Loss: 40.0000\n",
      "Epoch [980/1000], Loss: 40.0000\n",
      "Epoch [990/1000], Loss: 40.0000\n",
      "Epoch [1000/1000], Loss: 40.0000\n"
     ]
    }
   ],
   "source": [
    "# Train the model for 1000 epochs\n",
    "loss = fit_return_loss(1000, model, loss_fn, opt, train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1]], dtype=torch.int32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.int()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
